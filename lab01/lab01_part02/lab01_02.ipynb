{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/manorlf/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import nltk as nltk\n",
    "import math\n",
    "import operator\n",
    "import ast\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../../data/estadao_noticias_eleicao.csv\")\n",
    "df = df.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>str_busca</th>\n",
       "      <th>google</th>\n",
       "      <th>busca_binaria</th>\n",
       "      <th>tf</th>\n",
       "      <th>tfidf</th>\n",
       "      <th>bm25</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>segundo turno</td>\n",
       "      <td>[1062, 1942, 2161, 2078, 2073]</td>\n",
       "      <td>[2048, 1, 2049, 2050, 4096]</td>\n",
       "      <td>[2744, 7, 2112, 7672, 2388]</td>\n",
       "      <td>[2744, 2112, 7672, 1235, 2388]</td>\n",
       "      <td>[2744, 2112, 7672, 2388, 2178]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lava jato</td>\n",
       "      <td>[616, 164, 1734, 163, 6716]</td>\n",
       "      <td>[3, 13, 15, 27, 6177]</td>\n",
       "      <td>[163, 353, 2807, 127, 359]</td>\n",
       "      <td>[163, 353, 2807, 127, 359]</td>\n",
       "      <td>[163, 353, 2807, 127, 359]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>projeto de lei</td>\n",
       "      <td>[2853, 275, 978, 7092, 3171]</td>\n",
       "      <td>[3584, 6145, 8194, 8706, 6660]</td>\n",
       "      <td>[7, 3942, 7017, 1250, 6942]</td>\n",
       "      <td>[2232, 6461, 2853, 3171, 3942]</td>\n",
       "      <td>[2232, 6461, 3171, 2853, 3170]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>compra de voto</td>\n",
       "      <td>[2200, 8615, 2265, 7746, 82]</td>\n",
       "      <td>[7424, 2178, 6531, 5122, 2311]</td>\n",
       "      <td>[3942, 7017, 5129, 2047, 748]</td>\n",
       "      <td>[7343, 7293, 6791, 3942, 2047]</td>\n",
       "      <td>[7343, 7293, 6791, 7329, 8615]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ministério público</td>\n",
       "      <td>[64, 6652, 164, 6550, 8615]</td>\n",
       "      <td>[8194, 7, 4104, 8201, 4109]</td>\n",
       "      <td>[6798, 8018, 6244, 6965, 6550]</td>\n",
       "      <td>[6798, 8018, 6244, 6965, 6550]</td>\n",
       "      <td>[6798, 8018, 6244, 6965, 6550]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            str_busca                          google  \\\n",
       "0       segundo turno  [1062, 1942, 2161, 2078, 2073]   \n",
       "1           lava jato     [616, 164, 1734, 163, 6716]   \n",
       "2      projeto de lei    [2853, 275, 978, 7092, 3171]   \n",
       "3      compra de voto    [2200, 8615, 2265, 7746, 82]   \n",
       "4  ministério público     [64, 6652, 164, 6550, 8615]   \n",
       "\n",
       "                    busca_binaria                              tf  \\\n",
       "0     [2048, 1, 2049, 2050, 4096]     [2744, 7, 2112, 7672, 2388]   \n",
       "1           [3, 13, 15, 27, 6177]      [163, 353, 2807, 127, 359]   \n",
       "2  [3584, 6145, 8194, 8706, 6660]     [7, 3942, 7017, 1250, 6942]   \n",
       "3  [7424, 2178, 6531, 5122, 2311]   [3942, 7017, 5129, 2047, 748]   \n",
       "4     [8194, 7, 4104, 8201, 4109]  [6798, 8018, 6244, 6965, 6550]   \n",
       "\n",
       "                            tfidf                            bm25  \n",
       "0  [2744, 2112, 7672, 1235, 2388]  [2744, 2112, 7672, 2388, 2178]  \n",
       "1      [163, 353, 2807, 127, 359]      [163, 353, 2807, 127, 359]  \n",
       "2  [2232, 6461, 2853, 3171, 3942]  [2232, 6461, 3171, 2853, 3170]  \n",
       "3  [7343, 7293, 6791, 3942, 2047]  [7343, 7293, 6791, 7329, 8615]  \n",
       "4  [6798, 8018, 6244, 6965, 6550]  [6798, 8018, 6244, 6965, 6550]  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feedback\n",
    "\n",
    "feed_back = pd.read_csv('gabarito.csv')\n",
    "feed_back\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Vector Space Retrieval Model</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "INVERTED_INDEX = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_term(term, data):\n",
    "    '''\n",
    "    Search term presence in data frame and calculate its term frequence. Also full fill the dictionary \n",
    "    with the result for future new searchs.\n",
    "            \n",
    "    ARGS:\n",
    "        term: String with single word, that word is the term to be seach in the data frame.\n",
    "        data: Data frame, where the terms will be search.\n",
    "        \n",
    "    RETURN:\n",
    "        tuple (int n, list l): where n is the total of documents witch the term is presente at least once\n",
    "                               l is a list of tuple (doc, tf), where doc is the notice id and tf is its \n",
    "                               term frequence.\n",
    "    '''\n",
    "    \n",
    "    result = []\n",
    "    cont = 0\n",
    "    \n",
    "    if (term in INVERTED_INDEX):\n",
    "        result = INVERTED_INDEX[term]\n",
    "    else:\n",
    "        \n",
    "        rows = data.shape[0]\n",
    "                \n",
    "        for doc in range(rows):\n",
    "            title = (data.loc[doc, 'titulo']).lower()\n",
    "            sub_title = (data.loc[doc, 'subTitulo']).lower()\n",
    "            content = (data.loc[doc, 'conteudo']).lower()\n",
    "                        \n",
    "            tf = 0\n",
    "            text = nltk.word_tokenize(title + ' ' + sub_title + ' ' + content)\n",
    "            i = 0\n",
    "            \n",
    "            while (i < len(text)):\n",
    "                if (text[i].lower() == term):\n",
    "                    tf += 1\n",
    "                    exist = True\n",
    "                i += 1\n",
    "            if (tf):\n",
    "                id_notice = data.loc[doc, 'idNoticia']\n",
    "                result.append((id_notice, tf))\n",
    "                cont += 1\n",
    "        INVERTED_INDEX[term] = (cont, result)\n",
    "    \n",
    "    return INVERTED_INDEX[term]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def intersection(query, data): \n",
    "    '''\n",
    "    To get a dictionary with all notice id and its terms frequency in a list representing for all terms\n",
    "    of the query.\n",
    "        \n",
    "    ARGS:\n",
    "        query: String with the with words or terms to be seach\n",
    "        data: Data frame, where the terms will be search.\n",
    "        \n",
    "    RETURN:\n",
    "        {doc:l} doc is the notice id and l is a list with the tf (term frequency) of each term in the query\n",
    "        \n",
    "    '''\n",
    "    q = query.split()\n",
    "    dic_docs = dict()\n",
    "    \n",
    "    for term in q:\n",
    "        term_set = search_term(term, data)\n",
    "        for doc in term_set[1]:\n",
    "            if (doc[0] in dic_docs):\n",
    "                dic_docs[doc[0]].append(doc[1])\n",
    "            else:\n",
    "                dic_docs[doc[0]] = [doc[1]]\n",
    "    return dic_docs\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>VSM Binary</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vsm_binary(query, data):\n",
    "    '''\n",
    "    Virtual Space Model: Binary\n",
    "        Search for a specific query in a data frame with binary logic. \n",
    "        But here we use a intersection, all notice id needs to be in all docs simultaneously\n",
    "        \n",
    "    ARGS:\n",
    "        query: String with the with words or terms to be seach\n",
    "        data: Data frame, where the terms will be search.\n",
    "        \n",
    "    RETURN:\n",
    "        List of tuple (doc, tf), where doc repersents the notice id and tf its term frequency\n",
    "    \n",
    "    '''\n",
    "    q = query.split()\n",
    "    dic_docs = intersection(query,data)\n",
    "   \n",
    "    result = []\n",
    "    \n",
    "    for doc in dic_docs.keys():\n",
    "        if (len(dic_docs[doc]) == len(q)):\n",
    "            result.append((doc, len(q)))\n",
    "            \n",
    "    return result    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>VSM TF</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vsm_tf(query, data):\n",
    "    '''\n",
    "    Improved VSM with Term Frequency Weighting\n",
    "        Search for a specific query in a data frame with TF logic. For each term in the query it will result,,\n",
    "        a notice id with the sum of tf for each doc witch it's is present the term.\n",
    "        But here we use a intersection, all notice id needs to be in all docs simultaneously.\n",
    "        \n",
    "    ARGS:\n",
    "        query: String with the with words or terms to be seach\n",
    "        data: Data frame, where the terms will be search.\n",
    "        \n",
    "    RETURN:\n",
    "        List of tuple (doc, tf), where doc repersents the notice id and tf its term frequency\n",
    "    \n",
    "    '''\n",
    "    q = query.split()\n",
    "    dic_docs = intersection(query,data)\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    for doc in dic_docs.keys():\n",
    "        if (len(dic_docs[doc]) == len(q)):\n",
    "            result.append((doc, sum(dic_docs[doc])))\n",
    "            \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>VSM TF-IDF</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vsm_tf_idf(query, data):\n",
    "    '''\n",
    "    Improvement of Vector Placement: Adding Inverse Document Frequency (IDF)\n",
    "    IDF(W) = log[(M+1)/k]\n",
    "        W: Word\n",
    "        M: Total of Numbers in colection\n",
    "        K: Total of documents containing  W (Doc Frequency )\n",
    "        \n",
    "    Trying to improve search using TF-IDF. For each term it will calculate IDF and multiply for tf, and then sum\n",
    "    in order to improve Weighting.\n",
    "    Here we use a intersection, all docs selected needs to have the term at least once.\n",
    "        \n",
    "    ARGS:\n",
    "        query: String with the with words or terms to be seach.\n",
    "        data: Data frame, where the terms will be search.\n",
    "        \n",
    "    RETURN:\n",
    "        List of tuple (doc, tf), where doc repersents the notice id and tf-idf calc.\n",
    "    \n",
    "    '''\n",
    "    q = query.split()\n",
    "    dic_docs = intersection(query, data)\n",
    "    \n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    for doc in dic_docs.keys():\n",
    "        tf_idf_sum = 0\n",
    "        if (len(dic_docs[doc]) == len(q)):\n",
    "            for i in range(len(q)):\n",
    "                tf_idf_sum += dic_docs[doc][i] * idf(q[i], data)\n",
    "            result.append((doc, tf_idf_sum))\n",
    "            \n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def idf(term, data):\n",
    "    '''\n",
    "    Calculates IDF of a Word.\n",
    "        IDF(W) = log[(M+1)/k]\n",
    "            W: Word\n",
    "            M: Total of Numbers in colection\n",
    "            K: Total of documents containing  W (Doc Frequency )\n",
    "    ARGS:\n",
    "        term: Term or word to be calculated the idf.\n",
    "        data: Data frame.\n",
    "    \n",
    "    RETURN:\n",
    "        IDF for term.\n",
    "    '''\n",
    "    size = search_term(term, data)[0]\n",
    "    total_docs = data.shape[0]\n",
    "    calc = math.log((total_docs - 1)/ size)\n",
    "    \n",
    "    return calc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>BM25</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vsm_bm25(query, data):\n",
    "    '''\n",
    "    BM25 Transformation \n",
    "        – has an upper bound\n",
    "        – is robust and effective \n",
    "     \n",
    "    ARGS:\n",
    "        query: String with the with words or terms to be seach.\n",
    "        data: Data frame, where the terms will be search.\n",
    "    \n",
    "    RETURN:\n",
    "         List of tuple (doc, tf), where doc repersents the notice id and its BM25.\n",
    "    '''\n",
    "    q = query.split()\n",
    "    dic_docs = intersection(query, data)\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    for doc in dic_docs.keys():\n",
    "        bm25_sum = 0\n",
    "        if (len(dic_docs[doc]) == len(q)):\n",
    "            for i in range(len(q)):\n",
    "                bm25_sum += bm25(dic_docs[doc][i])\n",
    "            result.append((doc, bm25_sum))\n",
    "            \n",
    "    return result\n",
    "    \n",
    "    \n",
    "def bm25(tf):\n",
    "    '''\n",
    "    Calculates the BM25 for a specific tf\n",
    "    \n",
    "    '''\n",
    "    k = 5\n",
    "    calc = ((k + 1) * tf)/(tf + k)\n",
    "    return calc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Mean Average Precision (MAP)</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def apk(actual, predicted, k=10):\n",
    "    \"\"\"\n",
    "    Computes the average precision at k.\n",
    "    This function computes the average prescision at k between two lists of\n",
    "    items.\n",
    "    Parameters\n",
    "    ----------\n",
    "    actual : list\n",
    "             A list of elements that are to be predicted (order doesn't matter)\n",
    "    predicted : list\n",
    "                A list of predicted elements (order does matter)\n",
    "    k : int, optional\n",
    "        The maximum number of predicted elements\n",
    "    Returns\n",
    "    -------\n",
    "    score : double\n",
    "            The average precision at k over the input lists\n",
    "    \"\"\"\n",
    "    if len(predicted)>k:\n",
    "        predicted = predicted[:k]\n",
    "\n",
    "    score = 0.0\n",
    "    num_hits = 0.0\n",
    "\n",
    "    for i,p in enumerate(predicted):\n",
    "        if p in actual and p not in predicted[:i]:\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i+1.0)\n",
    "\n",
    "    if not actual:\n",
    "        return 0.0\n",
    "\n",
    "    return score / min(len(actual), k)\n",
    "\n",
    "def mapk(actual, predicted, k=10):\n",
    "    \"\"\"\n",
    "    Computes the mean average precision at k.\n",
    "    This function computes the mean average prescision at k between two lists\n",
    "    of lists of items.\n",
    "    Parameters\n",
    "    ----------\n",
    "    actual : list\n",
    "             A list of lists of elements that are to be predicted \n",
    "             (order doesn't matter in the lists)\n",
    "    predicted : list\n",
    "                A list of lists of predicted elements\n",
    "                (order matters in the lists)result\n",
    "    k : int, optional\n",
    "        The maximum number of predicted elements\n",
    "    Returns\n",
    "    -------\n",
    "    score : double\n",
    "            The mean average precision at k over the input lists\n",
    "    \"\"\"\n",
    "    return np.mean([apk(a,p,k) for a,p in zip(actual, predicted)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_five(query, model, data):\n",
    "    '''\n",
    "    Given a specific model it will make a search for the docs witch the terms are presents.\n",
    "    \n",
    "    ARGS:\n",
    "        query: String with the with words or terms to be seach.\n",
    "        model: Specif model for the search, should be binay, tf, tf-idf or bm25\n",
    "        data: Data frame, where the terms will be search. \n",
    "        \n",
    "    RETURN:\n",
    "        Top 5 documents for the search.\n",
    "        \n",
    "    '''\n",
    "    result = []\n",
    "    answer = []\n",
    "    if (model == 'binary'):\n",
    "        result = vsm_binary(query, data)\n",
    "    elif (model == 'tf'):\n",
    "        result = vsm_tf(query, data)\n",
    "    elif (model == 'tf-idf'):\n",
    "        result = vsm_tf_idf(query, data)\n",
    "    else:\n",
    "        result = vsm_bm25(query, data)\n",
    "    \n",
    "    result.sort(key=operator.itemgetter(1), reverse=True)\n",
    "    \n",
    "    for i in range(5):\n",
    "        answer.append(result[i][0])\n",
    "\n",
    "    \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>For testing propose only</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_google_result(feedback):\n",
    "    result = []\n",
    "    for doc in feedback.google:\n",
    "        result.append(ast.literal_eval(doc))\n",
    "    return result\n",
    "\n",
    "def get_seach_result(feedback, model):\n",
    "    '''\n",
    "    Get a result from the feedback in order to compare with the implemented algorithm.\n",
    "    \n",
    "    ARGS:\n",
    "        feedback: Feedback to compare with algoritm results.\n",
    "        model: Model to get specific result.\n",
    "    RESULT:\n",
    "        Feedback for the given model.\n",
    "    '''\n",
    "    \n",
    "    source = []\n",
    "    result = []\n",
    "    if (model == 'binary'):\n",
    "        source = feedback.busca_binaria\n",
    "    elif (model == 'tf'):\n",
    "        source = feedback.tf\n",
    "    elif (model == 'tf-idf'):\n",
    "        source = feedback.tfidf\n",
    "    else:\n",
    "        source = feedback.bm25\n",
    "    \n",
    "    for i in range(len(source)):\n",
    "        result.append(ast.literal_eval(source[i]))\n",
    "    return result\n",
    "\n",
    "def get_str_search(feedback, model, data):\n",
    "    result = []\n",
    "    for i in range(5):\n",
    "        result.append(top_five(feedback.str_busca[i], model, data))\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>VSM BINARY PRECISION</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Precision: 0.240\n",
      "Precision Google: 0.040\n"
     ]
    }
   ],
   "source": [
    "result = get_str_search(feed_back, 'binary', df)\n",
    "answer = get_seach_result(feed_back, 'binary')\n",
    "google = get_google_result(feed_back)\n",
    "   \n",
    "print('Binary Precision: %.3f' % (mapk(answer, result, k=5)))\n",
    "print('Precision Google: %.3f' % (mapk(google, result, k=5)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>VSM TF PRECISION</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF Precision: 1.000\n",
      "Precision Google: 0.048\n"
     ]
    }
   ],
   "source": [
    "tf_result = get_str_search(feed_back, 'tf', df)\n",
    "answer = get_seach_result(feed_back, 'tf')\n",
    "google = get_google_result(feed_back)\n",
    "\n",
    "print('TF Precision: %.3f' % (mapk(answer, tf_result, k=5)))\n",
    "print('Precision Google: %.3f' % (mapk(google, tf_result, k=5)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>VSM TF-IDF PRECISION</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Precision: 0.761\n",
      "Precision Google: 0.084\n"
     ]
    }
   ],
   "source": [
    "tf_idf_result = get_str_search(feed_back, 'tf-idf', df)\n",
    "answer = get_seach_result(feed_back, 'tf-idf')\n",
    "google = get_google_result(feed_back)\n",
    "\n",
    "print('TF-IDF Precision: %.3f' % (mapk(answer, tf_idf_result, k=5)))\n",
    "print('Precision Google: %.3f' % (mapk(google, tf_idf_result, k=5)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>VSM BM25 PRECISION</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BM25 Precision: 0.737\n",
      "Precision Google: 0.118\n"
     ]
    }
   ],
   "source": [
    "bm25_result = get_str_search(feed_back, 'bm25', df)\n",
    "answer = get_seach_result(feed_back, 'bm25')\n",
    "google = get_google_result(feed_back)\n",
    "\n",
    "print('BM25 Precision: %.3f' % (mapk(answer, bm25_result, k=5)))\n",
    "print('Precision Google: %.3f' % (mapk(google, bm25_result, k=5)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Avaliation</h1>\n",
    "\n",
    "<p>\n",
    "\n",
    "For all result, the best output was by VSM TF witch gave a precision of 1.0, but the google precision was not that good with 0.048.\n",
    "\n",
    "The best google precision was by VSM BM25 with 0.118, also showed a resonable result with 0.737.\n",
    "\n",
    "</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
