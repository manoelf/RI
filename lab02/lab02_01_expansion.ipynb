{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix and Vocabulary Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/josemsf/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to /home/josemsf/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from scipy import sparse\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk import bigrams\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "news = pd.read_csv(\"../data/estadao_noticias_eleicao.csv\", encoding=\"utf-8\")\n",
    "news = news.fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "content = news.titulo + \" \" + news.subTitulo + \" \" + news.conteudo\n",
    "content = content.fillna(\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Generating a Co-occurence Matrix</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def co_occurrence_matrix(corpus):\n",
    "    '''\n",
    "        By: https://github.com/allansales\n",
    "        Source: https://github.com/allansales/information-retrieval/tree/master/Lab%202\n",
    "    '''\n",
    "    vocab = set(corpus)\n",
    "    vocab = list(vocab)\n",
    "    n = len(vocab)\n",
    "   \n",
    "    vocab_to_index = {word:i for i, word in enumerate(vocab)}\n",
    "    \n",
    "    bi_grams = list(bigrams(corpus))\n",
    "\n",
    "    bigram_freq = nltk.FreqDist(bi_grams).most_common(len(bi_grams))\n",
    "\n",
    "    I=list()\n",
    "    J=list()\n",
    "    V=list()\n",
    "    \n",
    "    for bigram in bigram_freq:\n",
    "        current = bigram[0][1]\n",
    "        previous = bigram[0][0]\n",
    "        count = bigram[1]\n",
    "\n",
    "        I.append(vocab_to_index[previous])\n",
    "        J.append(vocab_to_index[current])\n",
    "        V.append(count)\n",
    "        \n",
    "    co_occurrence_matrix = sparse.coo_matrix((V,(I,J)), shape=(n,n))\n",
    "\n",
    "    return co_occurrence_matrix, vocab_to_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "tokens_lists = content.apply(lambda text: tokenizer.tokenize(text.lower()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopword_ = stopwords.words('portuguese')\n",
    "filtered_tokens = tokens_lists.apply(lambda tokens: [token for token in tokens if token not in stopword_])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transforming list of lists into one list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens = [token for tokens_list in filtered_tokens for token in tokens_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "matrix, vocab = co_occurrence_matrix(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Get the TOP 3 most frequent corelated word</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def top_3(word):\n",
    "    '''\n",
    "        Get the top 3 word more curelated whit the receavide as argument\n",
    "        \n",
    "        ARGS:\n",
    "            word: String which will search for other words most corealated with this one\n",
    "        RETURN:\n",
    "            List: List with the most corelated words\n",
    "    '''\n",
    "    word_id = vocab[word]\n",
    "    top = []\n",
    "    for i, j, k in zip(matrix.row, matrix.col, matrix.data):\n",
    "        if (i == word_id):\n",
    "            top.append(j)\n",
    "        if (len(top) == 3):\n",
    "            break\n",
    "    top = get_word_by_id(top)\n",
    "    return top\n",
    "\n",
    "\n",
    "def get_word_by_id(array):\n",
    "    '''\n",
    "        Transfor the list of IDs word into a array of words\n",
    "        \n",
    "        ARGS:\n",
    "            array: list of id words\n",
    "        RETURN:\n",
    "            A list with the words instead the ids\n",
    "    '''\n",
    "    result = ['','','']\n",
    "    for i in vocab.keys():\n",
    "        for j in range(len(array)):\n",
    "            if (vocab[i] == array[j]):\n",
    "                result[j] = i\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consult Bigram Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "consultable_matrix = matrix.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def consult_frequency(w1, w2):\n",
    "    return(consultable_matrix[vocab[w1],vocab[w2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1 = 'poucos'\n",
    "w2 = 'recursos'\n",
    "consult_frequency(w1, w2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Inverted Index</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "INVERTED_INDEX = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def search_term(term, data):\n",
    "    '''\n",
    "    Search term presence in data frame and calculate its term frequence. Also full fill the dictionary \n",
    "    with the result for future new searchs.\n",
    "            \n",
    "    ARGS:\n",
    "        term: String with single word, that word is the term to be seach in the data frame.\n",
    "        data: Data frame, where the terms will be search.\n",
    "        \n",
    "    RETURN:\n",
    "        tuple (int n, list l): where n is the total of documents witch the term is presente at least once\n",
    "                               l is a list of tuple (doc, tf), where doc is the notice id and tf is its \n",
    "                               term frequence.\n",
    "    '''\n",
    "    \n",
    "    result = []\n",
    "    cont = 0\n",
    "    \n",
    "    if (term in INVERTED_INDEX):\n",
    "        result = INVERTED_INDEX[term]\n",
    "    else:\n",
    "        \n",
    "        rows = data.shape[0]\n",
    "                \n",
    "        for doc in range(rows):\n",
    "            title = (data.loc[doc, 'titulo']).lower()\n",
    "            sub_title = (data.loc[doc, 'subTitulo']).lower()\n",
    "            content = (data.loc[doc, 'conteudo']).lower()\n",
    "                        \n",
    "            tf = 0\n",
    "            text = nltk.word_tokenize(title + ' ' + sub_title + ' ' + content)\n",
    "            i = 0\n",
    "            \n",
    "            while (i < len(text)):\n",
    "                if (text[i].lower() == term):\n",
    "                    tf += 1\n",
    "                    exist = True\n",
    "                i += 1\n",
    "            if (tf):\n",
    "                id_notice = data.loc[doc, 'idNoticia']\n",
    "                result.append((id_notice, tf))\n",
    "                cont += 1\n",
    "        INVERTED_INDEX[term] = (cont, result)\n",
    "    \n",
    "    return INVERTED_INDEX[term]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def disjunction(query, data): \n",
    "    '''\n",
    "    To get a dictionary with all notice id and its terms frequency in a list representing for all terms\n",
    "    of the query.\n",
    "        \n",
    "    ARGS:\n",
    "        query: String with the with words or terms to be seach\n",
    "        data: Data frame, where the terms will be search.\n",
    "        \n",
    "    RETURN:\n",
    "        {doc:l} doc is the notice id and l is a list with the tf (term frequency) of each term in the query\n",
    "        \n",
    "    '''\n",
    "    dic_docs = dict()\n",
    "    \n",
    "    for term in query:\n",
    "        term_set = search_term(term, data)\n",
    "        for doc in term_set[1]:\n",
    "            if (doc[0] in dic_docs):\n",
    "                dic_docs[doc[0]].append(doc[1])\n",
    "            else:\n",
    "                dic_docs[doc[0]] = [doc[1]]\n",
    "    return dic_docs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vsm_tf(query, data):\n",
    "    '''\n",
    "    Improved VSM with Term Frequency Weighting\n",
    "        Search for a specific query in a data frame with TF logic. For each term in the query it will result,\n",
    "        a notice id with the sum of tf for each doc witch it's is present the term.\n",
    "                \n",
    "    ARGS:\n",
    "        query: String with the with words or terms to be seach\n",
    "        data: Data frame, where the terms will be search.\n",
    "        \n",
    "    RETURN:\n",
    "        List of tuple (doc, tf), where doc repersents the notice id and tf its term frequency\n",
    "    \n",
    "    '''\n",
    "    dic_docs = disjunction(query,data)\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    for doc in dic_docs.keys():\n",
    "        result.append((doc, sum(dic_docs[doc])))\n",
    "            \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Analysis</h1>\n",
    "<p>For a seach with the term <i>petrobrás</i> we have got the following TOP 3 result as most frequents co-words:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 °: paulo\n",
      "2 °: é\n",
      "3 °: graça\n"
     ]
    }
   ],
   "source": [
    "top = top_3('petrobrás')\n",
    "for i, j in enumerate(top):\n",
    "    print('%d °: %s' % (i + 1, j))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The total of documents returned for the consult with only term <i>petrobrás</i> was about:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1043\n"
     ]
    }
   ],
   "source": [
    "print(len(vsm_tf([\"petrobrás\"], news)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>In another hand, for the expanded query, we have got the total of documents: </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6402\n"
     ]
    }
   ],
   "source": [
    "result = vsm_tf(top, news)\n",
    "print(len(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "    <p style=\"font-size:160%;\">Look it closer</p>\n",
    "    \n",
    "The question is: does it makes sense the result?\n",
    "As far we can see the answer is YES! \n",
    "That specific term 'petrobrás' we've got as result the word <i>paulo</i>, so what about?\n",
    "    First we should know what is <i>Petrobrás</i>, Petrobrás is a semi-public Brazilian multinational corporation in the petroleum industry headquartered in Rio de Janeiro, Brazil. \n",
    "Second what is <i>paulo</i>', it is a person name Paulo Roberto Costa who is a Brazilian engineer and former director of Petrobras Supply, between 2004 and 2012.\n",
    "So that makes totally sense as result for the expanded query!\n",
    "\n",
    "Taking a look to <b>precision</b> and <b>recall</b> we have a trade-off\n",
    "\n",
    "In the simple query we have less documents than in the expanded one, however those documents could be more related \n",
    "with the user search, so we should say that the we have a better <b>precision</b> but maybe not the best <b>recall</b>. For expanded query it will return a greater of documents but it will lose <b>precision</b>\n",
    "\n",
    "\n",
    "</p>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
